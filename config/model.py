__all__ = ["Qwen/Qwen1.5-14B-Chat-GPTQ-Int4", 
           "Qwen/Qwen1.5-32B-Chat-AWQ", 
           "internlm/internlm2-chat-7b", 
           "01-ai/Yi-1.5-9B-Chat",
           "modelscope/Yi-1.5-34B-Chat-AWQ",
           "CohereForAI/aya-23-8B",
           "meta-llama/Meta-Llama-3-8B-Instruct",
           "THUDM/glm-4-9b-chat",
           "Qwen/Qwen2-7B-Instruct",
           "google/gemma-1.1-7b-it",
           "mistralai/Mistral-7B-Instruct-v0.3",
           "microsoft/Phi-3-small-8k-instruct",
           "openbmb/MiniCPM-2B-dpo-bf16",
           "internlm/internlm2_5-7b-chat",
           "google/gemma-2-9b-it",
           "Qwen/Qwen1.5-110B-Chat-AWQ",
           "meta-llama/Meta-Llama-3-70B-Instruct",
           "mistralai/Mixtral-8x7B-Instruct-v0.1",
           "Qwen/Qwen2-72B-Instruct",
           "deepseek-ai/DeepSeek-V2-Lite-Chat",
           "alpindale/c4ai-command-r-plus-GPTQ",
           "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
           "Qwen/Qwen1.5-72B-Chat",
           "jarrelscy/Mixtral-8x22B-Instruct-v0.1-GPTQ-8bit",
           "Nexusflow/Athene-70B",
           "meta-llama/Meta-Llama-3.1-70B-Instruct",
           "casperhansen/mistral-large-instruct-2407-awq",
           "casperhansen/deepseek-coder-v2-instruct-awq", # todo: wait for vLLM to be updated
           "google/gemma-2-27b-it",
           ]

model_names = ["Qwen1.5-14B",
               "Qwen1.5-32B",
               "Internlm2-7B",
               "Yi-1.5-9B",
               "Yi-1.5-34B",
               "Aya-23-8B",
               "Llama-3-8B",
               "ChatGLM4-9B",
               "Qwen2-7B",
               "Gemma-1.1-7B",
               "Mistral-7B-v0.3",
               "Phi3-small",
               "MiniCPM-2B",
               "Internlm2.5-7B",
               "Gemma-2-9B",
               "Qwen1.5-110B",
               "Llama-3-70B",
               "Mixtral-8x7B",
               "Qwen2-72B",
               "DeepSeek-V2-Lite",
               "CommandR+",
               "DeepSeek-Coder-V2-Lite",
               "Qwen1.5-72B",
               "Mixtral-8x22B",
               "Athene-70B",
               "Llama-3.1-70B",
               "Mistral-Large-2",
               "DeepSeek-Coder-V2", # todo: wait for vLLM to be updated
               "Gemma-2-27B",
               ]
